{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google\n",
    "\n",
    "## Extraccion de locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=pd.read_csv('../datasets/csv/G_ulta_beauty.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica la ruta de la carpeta que contiene los archivos .parquet particionados\n",
    "ruta_carpeta_parquet = '../datasets/parquets/metadatos Google'\n",
    "\n",
    "# Lista para almacenar DataFrames de cada partición\n",
    "dataframes = []\n",
    "\n",
    "# Itera sobre los archivos .parquet en la carpeta\n",
    "for root, dirs, files in os.walk(ruta_carpeta_parquet):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            # Construye la ruta completa del archivo\n",
    "            ruta_completa = os.path.join(root, file)\n",
    "            \n",
    "            # Lee el archivo .parquet y agrega el DataFrame a la lista\n",
    "            df_particion = pd.read_parquet(ruta_completa)\n",
    "            dataframes.append(df_particion)\n",
    "\n",
    "# Concatena todos los DataFrames en uno solo\n",
    "df_google = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar por el valor 'ulta beauty' en la columna 'name'\n",
    "df_google = df_google[df_google['name'] == 'Ulta Beauty']\n",
    "# Eliminamos columnas innecesarias\n",
    "df_google = df_google[[\"gmap_id\",'url',\"latitude\",\"longitude\",\"avg_rating\",\"State\", \"state\", \"address\"]]\n",
    "# Eliminamos Duplicados\n",
    "df_google.drop_duplicates(inplace=True)\n",
    "# Se exporta\n",
    "df_google.to_csv('../datasets/csv/G_ulta_establishments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalamos extension para coneccion\n",
    "pip install apify-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar API Y datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_client import ApifyClient\n",
    "\n",
    "apify_client = ApifyClient('apify_api_USgUEX5UbJ4Z0YMJoKF4S1MYt9mCmw4mmrzs')\n",
    "\n",
    "# Fetch results from the actor's default dataset\n",
    "dataset = apify_client.dataset('IwxKoLSghgggI6jm1')\n",
    "\n",
    "#Extraigo los datos\n",
    "dataset_items=dataset.list_items()\n",
    "\n",
    "#Los paso a dataframe\n",
    "data_API=pd.DataFrame(data=dataset_items.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'searchString', 'rank', 'searchPageUrl', 'searchPageLoadedUrl',\n",
       "       'isAdvertisement', 'title', 'subTitle', 'description', 'price',\n",
       "       'categoryName', 'address', 'neighborhood', 'street', 'city',\n",
       "       'postalCode', 'state', 'countryCode', 'website', 'phone',\n",
       "       'phoneUnformatted', 'claimThisBusiness', 'location', 'locatedIn',\n",
       "       'plusCode', 'menu', 'totalScore', 'permanentlyClosed',\n",
       "       'temporarilyClosed', 'placeId', 'categories', 'cid', 'reviewsCount',\n",
       "       'reviewsDistribution', 'imagesCount', 'imageCategories', 'scrapedAt',\n",
       "       'reserveTableUrl', 'googleFoodUrl', 'hotelStars', 'hotelDescription',\n",
       "       'checkInDate', 'checkOutDate', 'similarHotelsNearby',\n",
       "       'hotelReviewSummary', 'hotelAds', 'popularTimesLiveText',\n",
       "       'popularTimesLivePercent', 'popularTimesHistogram', 'openingHours',\n",
       "       'peopleAlsoSearch', 'placesTags', 'reviewsTags', 'additionalInfo',\n",
       "       'gasPrices', 'questionsAndAnswers', 'updatesFromCustomers',\n",
       "       'webResults', 'orderBy', 'imageUrls', 'reviews'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_API.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar pd.json_normalize para aplanar la columna 'location'\n",
    "data_API = pd.concat([data_API, pd.json_normalize(data_API['location'])], axis=1)\n",
    "\n",
    "# Eliminar la columna 'location' si ya no la necesitas\n",
    "data_API = data_API.drop('location', axis=1)\n",
    "\n",
    "# Me quedo con las columnas de interes\n",
    "data_API=data_API[['url','searchString', 'state', 'lat', 'lng', 'reviews']]\n",
    "\n",
    "# Quito los diccionarios que estan en listas\n",
    "data_API = data_API.explode('reviews')\n",
    "\n",
    "#Reseteo los indices\n",
    "data_API=data_API.reset_index(drop=True)\n",
    "\n",
    "# Utilizo json_normalize para expandir la columna 'reviews'\n",
    "df_expanded  = pd.json_normalize(data_API['reviews'])\n",
    "\n",
    "# Combina el DataFrame expandido con el DataFrame original\n",
    "df_result = pd.concat([data_API, df_expanded], axis=1).drop(columns=['reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renombrar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borramos nuevamente las columnas innecesarias\n",
    "df_result=df_result[['url', 'reviewerId',  'stars', 'text', 'publishedAtDate',  'name', 'lat', 'lng', 'state']]\n",
    "\n",
    "# Y las renombramos\n",
    "df_result.rename(columns={'url':'business_url','reviewerId':'user_id',  'lat': 'latitude','lng': 'longitude', 'state':'state_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12376 entries, 0 to 12375\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   business_url     12376 non-null  object \n",
      " 1   user_id          0 non-null      float64\n",
      " 2   stars            12362 non-null  float64\n",
      " 3   text             8137 non-null   object \n",
      " 4   publishedAtDate  12362 non-null  object \n",
      " 5   name             12362 non-null  object \n",
      " 6   latitude         12373 non-null  float64\n",
      " 7   longitude        12373 non-null  float64\n",
      " 8   state_name       12373 non-null  object \n",
      "dtypes: float64(4), object(5)\n",
      "memory usage: 870.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = \"https://api.apify.com/v2/datasets/W09Q96b7CANtZblZ2/items?token=apify_api_USgUEX5UbJ4Z0YMJoKF4S1MYt9mCmw4mmrzs&unwind=reviews&fields=placeId,postalCode,state,city,reviews&omit=textTranslated,publishAt,likesCount,reviewId,reviewUrl,reviewerUrl,reviewerPhotoUrl,reviewerNumberOfReviews,isLocalGuide,rating,reviewImageUrls,reviewContext,reviewDetailedRating\"\n",
    "\n",
    "# Realizar una solicitud GET a la API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa (código de estado 200)\n",
    "if response.status_code == 200:\n",
    "    # Los datos de la API están en formato JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Ahora puedes trabajar con los datos, por ejemplo, imprimirlos\n",
    "    print(data)\n",
    "else:\n",
    "    # Si la solicitud no fue exitosa, imprimir el código de estado\n",
    "    print(f\"Error al obtener datos. Código de estado: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=pd.DataFrame(data=data)\n",
    "datos.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraigamos el Id de los locales para hacer el scrapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
