# Definir el nombre del proyecto
export PROJECT=windy-tiger-410421

# Establecer el proyecto actual en la configuración de gcloud
gcloud config set project $PROJECT

# Crear un directorio llamado "dataflow"
mkdir dataflow

# Cambiar al directorio "dataflow"
cd dataflow

# Copiar archivos desde un bucket de Google Cloud Storage a la carpeta local
gsutil cp -r gs://ultabeauty .

cd ultabeauty/

# Construir una imagen de Docker llamada "contenedorprueba" desde el Dockerfile en el directorio actual
docker build -t reviewsimage .

# Ejecutar un contenedor basado en la imagen "contenedorprueba"
docker run -it reviewsimage

# Ejecutar un script de Python llamado "pipeline.py" utilizando Dataflow en Google Cloud
python pipeline_apache_beam.py \
  --project=$PROJECT \  # Proyecto de Google Cloud
  --region=us-central1 \  # Región de ejecución
  --runner=DataflowRunner \  # Tipo de runner (Dataflow en este caso)
  --machine_type=e2-standard-2 \  # Tipo de máquina virtual utilizada por Dataflow
  --save_main_session \ # Indicar a Dataflow que guarde el estado principal de la sesión
  --temp_location=gs://ultabeauty/tmp

python pipeline_apache_beam.py \
  --project=$PROJECT \
  --region=us-central1 \
  --runner=DataflowRunner \
  --machine_type=e2-standard-2 \
  --save_main_session \
  --temp_location=gs://ultabeauty/tmp